{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the US EPA's 2019 National Emissions Inventory\n",
    "\n",
    "This notebook contains a work-in-progress effort for processing the 2019 NEI using the [Julia language](https://julialang.org/\n",
    "\n",
    "First, we need to load the packages that we will use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'pyproj' from 'C:\\\\Users\\\\hemamipo\\\\.julia\\\\conda\\\\3\\\\x86_64\\\\lib\\\\site-packages\\\\pyproj\\\\__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revise\n",
    "using CSV\n",
    "using DataFrames\n",
    "using DBFTables\n",
    "using ZipFile\n",
    "using Shapefile\n",
    "using GeometryBasics\n",
    "using LibGEOS\n",
    "using GeoInterface\n",
    "using GeoFormatTypes\n",
    "using Rasters\n",
    "using ArchGDAL\n",
    "using Proj\n",
    "using Extents\n",
    "using PyCall\n",
    "using JLD2\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "using Base.Threads\n",
    "using Dates\n",
    "using Printf\n",
    "using Plots\n",
    "using JSON\n",
    "using HTTP\n",
    "using Unitful\n",
    "using FilePathsBase\n",
    "using CodecZlib\n",
    "\n",
    "pyproj = pyimport(\"pyproj\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set download directory\n",
    "\n",
    "Next, we need to choose a directory on our computer to download the emissions data to. You will probably need to change the directory below to a location that exists on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:/Users/hemamipo/OneDrive - University of Illinois - Urbana/Research-UIUC/Tessum Group/Tasks/NEI_Processor/2019nei\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/Users/$(ENV[\"USER\"])/data/2019nei\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download emissions files from the EPA web server\n",
    "\n",
    "In this step, we download and unzip all of the emissions files.\n",
    "\n",
    "You only need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_CMV_c1c2_inventory_16mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_CMV_c3_inventory_16mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_cem_inventory_10mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_fertilizer_inventory_04apr2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_fires_inventory_10mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_nonpoint_inventory_10mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_nonroad_inventory_10mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_onroad_SMOKE-MOVES_emissions_FF10_allHAPs_30nov2021_v0.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_onroad_activity_10mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_oth_inventory_17mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/2019emissions/2019ge_point_inventory_10mar2023.zip\",\n",
    "    \"https://gaftp.epa.gov/Air/emismod/2019/ancillary_data/2019ge_gridding_ge_dat_10mar2023.zip\"\n",
    "]\n",
    "\n",
    "for url in urls\n",
    "    println(\"Downloading $(url)\")\n",
    "    http_response = HTTP.get(url, require_ssl_verification = false)\n",
    "    r = ZipFile.Reader(IOBuffer(http_response.body));\n",
    "    for f in r.files\n",
    "        println(\"Filename: $(f.name)\")\n",
    "        file_path = joinpath(dir, f.name)\n",
    "        if f.name[end] == '/'\n",
    "            mkpath(file_path)\n",
    "            continue\n",
    "        end\n",
    "        mkpath(dirname(file_path))\n",
    "        open(file_path, \"w\") do o\n",
    "            write(o, read(f))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "\n",
    "This next cell contains some code that we will use to load and process the emissions. It will eventually be moved into a separate software library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_locIndex (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const tonperyear = 907.185u\"kg\" / 31_536_000u\"s\"\n",
    "const tonpermonth = 907.185u\"kg\" / 2_628_288u\"s\"\n",
    "const foot = (1/3.28084)u\"m\"\n",
    "kelvin(F) = ((F âˆ’ 32.0) * 5.0/9.0 + 273.15) * u\"K\"\n",
    "\n",
    "abstract type EmissionsDataFrame end\n",
    "\n",
    "# SurrogateSpec holds surrogate specification data\n",
    "struct SurrogateSpec\n",
    "    Region::String\n",
    "    Name::String\n",
    "    Code::Int\n",
    "    DataShapefile::String\n",
    "    DataAttribute::String\n",
    "    WeightShapefile::String\n",
    "    Details::String\n",
    "    BackupSurrogateNames::Vector{String}\n",
    "    WeightColumns::Vector{String}\n",
    "    WeightFactors::Vector{Float64}\n",
    "    FilterFunction::String\n",
    "    MergeNames::Vector{String}\n",
    "    MergeMultipliers::Vector{Float64}\n",
    "end\n",
    "\n",
    "# GridDef specifies the grid that we are allocating the emissions to.\n",
    "struct GridDef\n",
    "    Name::String\n",
    "    Nx::Int\n",
    "    Ny::Int\n",
    "    SR::String\n",
    "    Cells::Vector{LibGEOS.Polygon}\n",
    "    Extent::Vector{Vector{Tuple{Float64, Float64}}}\n",
    "end\n",
    "\n",
    "# SpatialProcessor spatializes emissions records.\n",
    "struct SpatialProcessor\n",
    "    SrgSpecs::Vector{SurrogateSpec}\n",
    "    Grids::GridDef\n",
    "    GridRef::DataFrame\n",
    "    InputSR::String\n",
    "    MatchFullSCC::Bool\n",
    "    MemCacheSize::Int\n",
    "    MaxMergeDepth::Int\n",
    "end\n",
    "\n",
    "# Config holds configuration data.\n",
    "struct Config\n",
    "    f_gridRef::Vector{String}\n",
    "    SrgSpec::String\n",
    "    SrgShapefileDirectory::String\n",
    "    InputSR::String\n",
    "    OutputSR::String\n",
    "    GridFile::String\n",
    "    GridName::String\n",
    "    Counties::String\n",
    "    EmisShp::String\n",
    "end\n",
    "\n",
    "# IndexInfo holds grid index information for gridded emissions.\n",
    "struct IndexInfo\n",
    "    rows::Vector{Int}\n",
    "    cols::Vector{Int}\n",
    "    fracs::Vector{Float64}\n",
    "    inGrid::Bool\n",
    "    coveredByGrid::Bool\n",
    "end\n",
    "\n",
    "# Pollutants holds pollutant groups.\n",
    "Pollutants = Dict(\n",
    "    \"EVP__VOC\" => \"VOC\", \"EXH__VOC\" => \"VOC\", \"VOC\" => \"VOC\", \"VOC_INV\" => \"VOC\",\n",
    "    \"NOX\" => \"NOX\",\n",
    "    \"NH3\" => \"NH3\",\n",
    "    \"SO2\" => \"SO2\",\n",
    "    \"BRK__PM25-PRI\" => \"PM25\", \"EXH__PM25-PRI\" => \"PM25\", \"EXH__PM2_5\" => \"PM25\", \n",
    "    \"PM25-PRI\" => \"PM25\", \"PM25TOTAL\" => \"PM25\", \"PM2_5\" => \"PM25\", \"TIR__PM25-PRI\" => \"PM25\",\n",
    "    \"BRK__PM2_5\" => \"PM25\", \"TIR__PM2_5\" => \"PM25\",\n",
    ")\n",
    "\n",
    "# https://www.cmascenter.org/smoke/documentation/4.8.1/html/ch08s02s04.html#d0e38214\n",
    "struct FF10NonPointDataFrame <: EmissionsDataFrame\n",
    "    df::DataFrame\n",
    "    \n",
    "    FF10NonPointDataFrame(df::DataFrame) = begin\n",
    "        if size(df, 2) != 45\n",
    "            throw(DimensionMismatch(\"FF10 nonpoint file should have 45 fields but instead has $(size(df,2))\"))\n",
    "        end\n",
    "        \n",
    "        rename!(df, [\"COUNTRY\", \"FIPS\", \"TRIBAL_CODE\", \"CENSUS_TRACT\", \"SHAPE_ID\", \"SCC\",\n",
    "            \"EMIS_TYPE\", \"POLID\", \"ANN_VALUE\",\n",
    "            \"ANN_PCT_RED\", \"CONTROL_IDS\", \"CONTROL_MEASURES\", \"CURRENT_COST\", \"CUMULATIVE_COST\", \"PROJECTION_FACTOR\",\n",
    "            \"REG_CODES\", \"CALC_METHOD\", \"CALC_YEAR\", \"DATE_UPDATED\", \"DATA_SET_ID\", \"JAN_VALUE\", \"FEB_VALUE\", \"MAR_VALUE\",\n",
    "            \"APR_VALUE\", \"MAY_VALUE\", \"JUN_VALUE\", \"JUL_VALUE\", \"AUG_VALUE\", \"SEP_VALUE\", \"OCT_VALUE\", \"NOV_VALUE\", \"DEC_VALUE\",\n",
    "            \"JAN_PCTRED\", \"FEB_PCTRED\", \"MAR_PCTRED\", \"APR_PCTRED\", \"MAY_PCTRED\", \"JUN_PCTRED\", \"JUL_PCTRED\", \"AUG_PCTRED\",\n",
    "            \"SEP_PCTRED\", \"OCT_PCTRED\", \"NOV_PCTRED\", \"DEC_PCTRED\", \"COMMENT\"])\n",
    "        \n",
    "        df[!, :FIPS] = begin\n",
    "            fips_transformed = String[]\n",
    "            for fips in df[!, :FIPS]\n",
    "                fips_str = string(fips)\n",
    "                if length(fips_str) == 6\n",
    "                    fips_str = fips_str[2:end]\n",
    "                end\n",
    "                push!(fips_transformed, lpad(fips_str, 5, '0'))\n",
    "            end\n",
    "            fips_transformed\n",
    "        end\n",
    "\n",
    "        df[!, :SCC] = [lpad(scc, 10, '0') for scc in string.(df[!, :SCC])]\n",
    "        \n",
    "        df.ANN_VALUE = df.ANN_VALUE * tonperyear\n",
    "        \n",
    "        df.JAN_VALUE = df.JAN_VALUE * tonpermonth\n",
    "        df.FEB_VALUE = df.FEB_VALUE * tonpermonth\n",
    "        df.MAR_VALUE = df.MAR_VALUE * tonpermonth\n",
    "        df.APR_VALUE = df.APR_VALUE * tonpermonth\n",
    "        df.MAY_VALUE = df.MAY_VALUE * tonpermonth\n",
    "        df.JUN_VALUE = df.JUN_VALUE * tonpermonth\n",
    "        df.JUL_VALUE = df.JUL_VALUE * tonpermonth\n",
    "        df.AUG_VALUE = df.AUG_VALUE * tonpermonth\n",
    "        df.SEP_VALUE = df.SEP_VALUE * tonpermonth\n",
    "        df.OCT_VALUE = df.OCT_VALUE * tonpermonth\n",
    "        df.NOV_VALUE = df.NOV_VALUE * tonpermonth\n",
    "        df.DEC_VALUE = df.DEC_VALUE * tonpermonth\n",
    "        \n",
    "        return new(df)\n",
    "    end\n",
    "end\n",
    "\n",
    "struct FF10NonRoadDataFrame <: EmissionsDataFrame\n",
    "    df::DataFrame\n",
    "    \n",
    "    FF10NonRoadDataFrame(df::DataFrame) = new(FF10NonPointDataFrame(df).df)\n",
    "end\n",
    "\n",
    "struct FF10OnRoadDataFrame <: EmissionsDataFrame\n",
    "    df::DataFrame\n",
    "    \n",
    "    FF10OnRoadDataFrame(df::DataFrame) = new(FF10NonPointDataFrame(df).df)\n",
    "end\n",
    "\n",
    "# https://www.cmascenter.org/smoke/documentation/4.8.1/html/ch08s02s08.html#sect_input_ptinv_ff10\n",
    "struct FF10PointDataFrame <: EmissionsDataFrame\n",
    "    df::DataFrame\n",
    "    \n",
    "    FF10PointDataFrame(df::DataFrame) = begin\n",
    "        if size(df, 2) != 77\n",
    "            throw(DimensionMismatch(\"FF10 point file should have 77 fields but instead has $(size(df,2))\"))\n",
    "        end\n",
    "        \n",
    "        rename!(df, [\n",
    "            \"COUNTRY\",\"FIPS\",\"TRIBAL_CODE\",\"FACILITY_ID\",\"UNIT_ID\",\"REL_POINT_ID\",\"PROCESS_ID\",\"AGY_FACILITY_ID\",\"AGY_UNIT_ID\",                \n",
    "            \"AGY_REL_POINT_ID\",\"AGY_PROCESS_ID\",\"SCC\",\"POLID\",\"ANN_VALUE\",\"ANN_PCT_RED\",\"FACILITY_NAME\",\"ERPTYPE\",\"STKHGT\",\n",
    "            \"STKDIAM\",\"STKTEMP\",\"STKFLOW\",\"STKVEL\",\"NAICS\",\"LONGITUDE\",\"LATITUDE\",\"LL_DATUM\",\"HORIZ_COLL_MTHD\",\"DESIGN_CAPACITY\",\n",
    "            \"DESIGN_CAPACITY_UNITS\",\"REG_CODES\",\"FAC_SOURCE_TYPE\",\"UNIT_TYPE_CODE\",\"CONTROL_IDS\",\"CONTROL_MEASURES\",\n",
    "            \"CURRENT_COST\",\"CUMULATIVE_COST\",\"PROJECTION_FACTOR\",\"SUBMITTER_FAC_ID\",\"CALC_METHOD\",\"DATA_SET_ID\",\"FACIL_CATEGORY_CODE\",\n",
    "            \"ORIS_FACILITY_CODE\",\"ORIS_BOILER_ID\",\"IPM_YN\",\"CALC_YEAR\",\"DATE_UPDATED\",\"FUG_HEIGHT\",\"FUG_WIDTH_XDIM\",\"FUG_LENGTH_YDIM\",\n",
    "            \"FUG_ANGLE\",\"ZIPCODE\",\"ANNUAL_AVG_HOURS_PER_YEAR\",\"JAN_VALUE\", \"FEB_VALUE\", \"MAR_VALUE\",\n",
    "            \"APR_VALUE\", \"MAY_VALUE\", \"JUN_VALUE\", \"JUL_VALUE\", \"AUG_VALUE\", \"SEP_VALUE\", \"OCT_VALUE\", \"NOV_VALUE\", \"DEC_VALUE\",\n",
    "            \"JAN_PCTRED\", \"FEB_PCTRED\", \"MAR_PCTRED\", \"APR_PCTRED\", \"MAY_PCTRED\", \"JUN_PCTRED\", \"JUL_PCTRED\", \"AUG_PCTRED\",\n",
    "            \"SEP_PCTRED\", \"OCT_PCTRED\", \"NOV_PCTRED\", \"DEC_PCTRED\", \"COMMENT\"    \n",
    "        ])\n",
    "        \n",
    "        df[!, :FIPS] = begin\n",
    "            fips_transformed = String[]\n",
    "            for fips in df[!, :FIPS]\n",
    "                fips_str = string(fips)\n",
    "                if length(fips_str) == 6\n",
    "                    fips_str = fips_str[2:end]\n",
    "                end\n",
    "                push!(fips_transformed, lpad(fips_str, 5, '0'))\n",
    "            end\n",
    "            fips_transformed\n",
    "        end\n",
    "\n",
    "        \n",
    "        df[!, :SCC] = [lpad(scc, 10, '0') for scc in string.(df[!, :SCC])]\n",
    "        \n",
    "        df.ANN_VALUE = df.ANN_VALUE * tonperyear\n",
    "        \n",
    "        df.JAN_VALUE = df.JAN_VALUE * tonpermonth\n",
    "        df.FEB_VALUE = df.FEB_VALUE * tonpermonth\n",
    "        df.MAR_VALUE = df.MAR_VALUE * tonpermonth\n",
    "        df.APR_VALUE = df.APR_VALUE * tonpermonth\n",
    "        df.MAY_VALUE = df.MAY_VALUE * tonpermonth\n",
    "        df.JUN_VALUE = df.JUN_VALUE * tonpermonth\n",
    "        df.JUL_VALUE = df.JUL_VALUE * tonpermonth\n",
    "        df.AUG_VALUE = df.AUG_VALUE * tonpermonth\n",
    "        df.SEP_VALUE = df.SEP_VALUE * tonpermonth\n",
    "        df.OCT_VALUE = df.OCT_VALUE * tonpermonth\n",
    "        df.NOV_VALUE = df.NOV_VALUE * tonpermonth\n",
    "        df.DEC_VALUE = df.DEC_VALUE * tonpermonth\n",
    "        \n",
    "        df.STKHGT = df.STKHGT * foot\n",
    "        df.STKDIAM = df.STKDIAM * foot\n",
    "        df.STKTEMP = kelvin.(df.STKTEMP)\n",
    "        df.STKFLOW = df.STKFLOW * foot * foot * foot / u\"s\"\n",
    "        df.STKVEL = df.STKVEL * foot / u\"s\"\n",
    "        \n",
    "        # Fill in missing parameters.\n",
    "        # Commented out for now.\n",
    "        #flowmissing = ismissing.(df.STKFLOW)\n",
    "        #df.STKFLOW[flowmissing] .= df.STKVEL[flowmissing] .* (df.STKDIAM[flowmissing] .* df.STKDIAM[flowmissing] / 4 * Ï€)\n",
    "        \n",
    "        #velmissing = ismissing.(df.STKVEL)\n",
    "        #df.STKVEL[velmissing] .= df.STKVEL[velmissing] ./ (df.STKDIAM[velmissing] .* df.STKDIAM[velmissing] / 4 * Ï€)\n",
    "        \n",
    "        return new(df)\n",
    "    end\n",
    "end\n",
    "\n",
    "strip_missing(x) = ismissing(x) ? \"\" : strip(x)\n",
    "\n",
    "# getCountry returns the Country associated with this code.\n",
    "function getCountry(code)\n",
    "    countryMap = Dict(\n",
    "        \"0\" => \"USA\",\n",
    "        \"1\" => \"Canada\",\n",
    "        \"2\" => \"Mexico\",\n",
    "        \"3\" => \"Cuba\",\n",
    "        \"4\" => \"Bahamas\",\n",
    "        \"5\" => \"Haiti\",\n",
    "        \"6\" => \"Dominican Republic\",\n",
    "        \"7\" => \"Global\"\n",
    "    )\n",
    "    return get(countryMap, code, \"Unknown\")\n",
    "end\n",
    "\n",
    "# read_grid reads grid file data.\n",
    "function read_grid(file_path)\n",
    "    data = DataFrame(COUNTRY=String[], FIPS=String[], SCC=String[], Surrogate=String[])\n",
    "    open(file_path) do file\n",
    "        for line in eachline(file)\n",
    "            if startswith(line, '#') || !contains(line, ';')\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            parts = split(line, ';', limit=3)\n",
    "\n",
    "            if length(parts) >= 3\n",
    "                surrogate_part = split(parts[3], '!', limit=2)[1]\n",
    "                fips = parts[1]\n",
    "                scc = parts[2]\n",
    "                surrogate = strip(surrogate_part)\n",
    "\n",
    "                country = \"USA\"\n",
    "                if length(fips) == 6\n",
    "                    country = getCountry(fips[1:1])\n",
    "                    fips = fips[2:end]\n",
    "                end\n",
    "\n",
    "                if length(scc) == 8\n",
    "                    scc = \"00\" * scc\n",
    "                end\n",
    "\n",
    "                push!(data, (COUNTRY=country, FIPS=fips, SCC=scc, Surrogate=surrogate))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "# getShapefilePath gets the shapefile path of a DataShapefile or WeightShapefile.\n",
    "function getShapefilePath(baseDir::String, ShapefileName::String, CheckShapefiles::Bool)::String\n",
    "    if CheckShapefiles\n",
    "        for (root, dirs, files) in walkdir(baseDir)\n",
    "            for file in files\n",
    "                if file == ShapefileName * \".shp\"\n",
    "                    return joinpath(root, file)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        @warn \"Shapefile not found: $(joinpath(baseDir, ShapefileName * \".shp\"))\"\n",
    "        return \"\"\n",
    "    else\n",
    "        return joinpath(baseDir, ShapefileName * \".shp\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# validateShapefile validates a shapefile.\n",
    "function validateShapefile(shapefilePath::String)\n",
    "    if shapefilePath != \"\"\n",
    "        try\n",
    "            shp = open(shapefilePath)\n",
    "            close(shp)\n",
    "            return true\n",
    "        catch e\n",
    "            @warn \"Failed to open shapefile: $shapefilePath; Error: $e\"\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "# readSrgSpecSMOKE reads a SMOKE formatted spatial surrogate specification file.\n",
    "# Results are returned as a map of surrogate specifications.\n",
    "function readSrgSpecSMOKE(SrgSpec::String, SrgShapefileDirectory::String, CheckShapefiles::Bool=false)\n",
    "    \n",
    "    df = CSV.File(SrgSpec; comment=\"#\", header=1, delim=',', quotechar='\"', escapechar='\"') |> DataFrame\n",
    "\n",
    "    srgs = SurrogateSpec[]\n",
    "\n",
    "    for row in eachrow(df)\n",
    "        WeightColumns = String[]\n",
    "        WeightFactors = Float64[]\n",
    "        MergeNames = String[]\n",
    "        MergeMultipliers = Float64[]\n",
    "        Region = get(row, 1, \"\") |> strip_missing\n",
    "        Name = get(row, 2, \"\") |> strip_missing\n",
    "        Code = get(row, 3, 0)\n",
    "        DataShapefile = get(row, 4, \"\") |> strip_missing\n",
    "        DataAttribute = get(row, 5, \"\") |> strip_missing\n",
    "        WeightShapefile = get(row, 6, \"\") |> strip_missing\n",
    "        WeightAttribute = get(row, 7, \"\") |> strip_missing\n",
    "        WeightFunction = get(row, 8, \"\") |> strip_missing\n",
    "        FilterFunction = get(row, 9, \"\") |> strip_missing\n",
    "        MergeFunction = get(row, 10, \"\") |> strip_missing\n",
    "        BackupSurrogateNames = [\n",
    "            strip(coalesce(get(row, 11, missing), \"\")), \n",
    "            strip(coalesce(get(row, 12, missing), \"\")), \n",
    "            strip(coalesce(get(row, 13, missing), \"\"))\n",
    "        ]\n",
    "        BackupSurrogateNames = filter(s -> !isempty(s), BackupSurrogateNames)\n",
    "        Details = get(row, 14, \"\") |> strip_missing\n",
    "\n",
    "        if WeightAttribute != \"none\" && WeightAttribute != \"\"\n",
    "            push!(WeightColumns, WeightAttribute)\n",
    "            push!(WeightFactors, 1.0)\n",
    "        end\n",
    "\n",
    "        if WeightFunction != \"\"\n",
    "            WeightFunctionParts = split(WeightFunction, \"+\")\n",
    "            for wf in WeightFunctionParts\n",
    "                mulFunc = split(wf, \"*\")\n",
    "                if length(mulFunc) == 1\n",
    "                    push!(WeightColumns, strip(mulFunc[1]))\n",
    "                    push!(WeightFactors, 1.0)\n",
    "                elseif length(mulFunc) == 2\n",
    "                    factor = tryparse(Float64, strip(mulFunc[1]))\n",
    "                    if isnothing(factor)\n",
    "                        error(\"Invalid weight factor in weight function: $wf\")\n",
    "                    end\n",
    "                    push!(WeightColumns, strip(mulFunc[2]))\n",
    "                    push!(WeightFactors, factor)\n",
    "                else\n",
    "                    error(\"Invalid weight function format: $wf\")\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if MergeFunction != \"none\" && MergeFunction != \"\"\n",
    "            parts = split(MergeFunction, \"+\")\n",
    "            for part in parts\n",
    "                components = split(part, \"*\")\n",
    "                if length(components) == 2\n",
    "                    Name = strip(components[2])\n",
    "                    val = tryparse(Float64, strip(components[1]))\n",
    "                    if isnothing(val)\n",
    "                        error(\"Invalid merge multiplier in merge function: $part\")\n",
    "                    end\n",
    "                    push!(MergeNames, Name)\n",
    "                    push!(MergeMultipliers, val)\n",
    "                else\n",
    "                    error(\"Invalid merge function format: $part\")\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if !isempty(DataShapefile)\n",
    "            DataShapefile = getShapefilePath(SrgShapefileDirectory, String(DataShapefile), CheckShapefiles)\n",
    "            isValidDataShapefile = validateShapefile(DataShapefile)\n",
    "        end\n",
    "            \n",
    "        if !isempty(WeightShapefile)\n",
    "            WeightShapefile = getShapefilePath(SrgShapefileDirectory, String(WeightShapefile), CheckShapefiles)\n",
    "            isValidWeightShapefile = validateShapefile(WeightShapefile)\n",
    "        end\n",
    "        \n",
    "        push!(srgs, SurrogateSpec(Region, Name, Code, DataShapefile, DataAttribute,\n",
    "                                        WeightShapefile, Details, BackupSurrogateNames,\n",
    "                                        WeightColumns, WeightFactors, FilterFunction,\n",
    "                                        MergeNames, MergeMultipliers))\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return srgs\n",
    "end\n",
    "\n",
    "# NewSpatialProcessor creates a new spatial processor.\n",
    "function NewSpatialProcessor(srgSpecs::Vector{SurrogateSpec}, grids::GridDef, gridRef::DataFrame, inputSR::String, matchFullSCC::Bool)\n",
    "    sp = SpatialProcessor(\n",
    "        srgSpecs,\n",
    "        grids,\n",
    "        gridRef,\n",
    "        inputSR,\n",
    "        matchFullSCC,\n",
    "        100,\n",
    "        10\n",
    "    )\n",
    "    return sp\n",
    "end\n",
    "\n",
    "# NewPolygon creates a new polygon from a string of coordinates.\n",
    "function NewPolygon(polygon_str::String, inputSR::String, outputSR::String)\n",
    "    points = Point{2, Float64}[]\n",
    "    regex = r\"X:(-?\\d+(\\.\\d+)?(e[+\\-]?\\d+)?) Y:(-?\\d+(\\.\\d+)?(e[+\\-]?\\d+)?)\"\n",
    "    trans = Proj.Transformation(inputSR, outputSR)\n",
    "    matches = collect(eachmatch(regex, polygon_str))\n",
    "    for match in matches\n",
    "        x1 = parse(Float64, match.captures[1])\n",
    "        y1 = parse(Float64, match.captures[4])\n",
    "        x, y = trans([x1, y1])\n",
    "        push!(points, Point(x, y))\n",
    "    end\n",
    "    return Polygon(points)\n",
    "end\n",
    "\n",
    "\n",
    "# NewGridIrregular creates a new irregular grid.\n",
    "function NewGridIrregular(name::String, filepath::String, inputSR::String, outputSR::String)\n",
    "    gridCells = Vector{LibGEOS.Polygon}()\n",
    "    gridExtent = Vector{Vector{Tuple{Float64, Float64}}}()\n",
    "    regex = r\"X:(-?\\d+(\\.\\d+)?(e[+\\-]?\\d+)?) Y:(-?\\d+(\\.\\d+)?(e[+\\-]?\\d+)?)\"\n",
    "    trans = Proj.Transformation(inputSR, outputSR)\n",
    "\n",
    "    open(filepath, \"r\") do file\n",
    "        content = read(file, String)\n",
    "        polygon_strs = split(replace(content, '\\n' => \"\"), \"]] [[\")\n",
    "\n",
    "        for polygon_str in polygon_strs\n",
    "            cleaned_str = replace(polygon_str, r\"[\\[\\]]\" => \"\")\n",
    "            matches = collect(eachmatch(regex, cleaned_str))\n",
    "\n",
    "            wkt_points_str = join([\n",
    "                let (x1, y1) = (parse(Float64, match.captures[1]), parse(Float64, match.captures[4])),\n",
    "                    (x, y) = trans([x1, y1])\n",
    "                in\n",
    "                    \"$(x) $(y)\"\n",
    "                end for match in matches\n",
    "            ], \", \")\n",
    "\n",
    "            polygon_wkt = \"POLYGON(($wkt_points_str))\"\n",
    "            polygon = LibGEOS.readgeom(polygon_wkt)\n",
    "            push!(gridCells, polygon)\n",
    "\n",
    "            polygonCoords = [Tuple(trans([parse(Float64, match.captures[1]), parse(Float64, match.captures[4])])) for match in matches]\n",
    "            push!(gridExtent, polygonCoords)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    Nx = 1\n",
    "    Ny = length(gridCells)\n",
    "    \n",
    "    return GridDef(name, Nx, Ny, outputSR, gridCells, gridExtent)\n",
    "end\n",
    "\n",
    "\n",
    "# setupSpatialProcessor reads in the necessary information to initialize\n",
    "# a processor for spatializing emissions, and then does so.\n",
    "function setupSpatialProcessor(c::Config)\n",
    "\n",
    "    srgSpecs = readSrgSpecSMOKE(c.SrgSpec, c.SrgShapefileDirectory, true)\n",
    "\n",
    "    gridRef = DataFrame(COUNTRY=String[], FIPS=String[], SCC=String[], Surrogate=String[])\n",
    "    for f in c.f_gridRef\n",
    "        gridRefTemp = read_grid(f)\n",
    "        append!(gridRef, gridRefTemp)\n",
    "    end\n",
    "\n",
    "    grids = NewGridIrregular(c.GridName, c.GridFile, c.OutputSR, c.OutputSR)\n",
    "\n",
    "    sp = NewSpatialProcessor(srgSpecs, grids, gridRef, c.InputSR, false)\n",
    "\n",
    "    return sp, grids\n",
    "end\n",
    "\n",
    "# findCountyPolygon finds county polygon based on location and counties shapefile.\n",
    "function findCountyPolygon(loc::String, countiesShapefile::String)\n",
    "    fips_match = match(r\"\\d+\", loc)\n",
    "    fips_code = fips_match.match\n",
    "\n",
    "    prj_path = replace(countiesShapefile, \".shp\" => \".prj\")\n",
    "    tgt_crs = open(prj_path, \"r\") do file\n",
    "        read(file, String)\n",
    "    end\n",
    "    dbf_path = replace(countiesShapefile, \".shp\" => \".dbf\")\n",
    "    shapes = Shapefile.Table(countiesShapefile)\n",
    "    dbf_data = DBFTables.Table(dbf_path)\n",
    "    attributes = DataFrame(dbf_data)\n",
    "\n",
    "    for (index, shape) in enumerate(shapes)\n",
    "        geoid = string(attributes[index, :GEOID])\n",
    "\n",
    "        if geoid == fips_code\n",
    "            countyPolygon = GeoInterface.convert(LibGEOS, shape.geometry)\n",
    "            return countyPolygon, tgt_crs\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return nothing, nothing\n",
    "end\n",
    "\n",
    "\n",
    "# GetIndex returns the row and column indices for a county or point in the grid.\n",
    "function GetIndex(grid::GridDef, gridCRS::String, emissionType::String; loc::String = \"\", countiesShapefile::String = \"\", x::Float64 = NaN, y::Float64 = NaN)\n",
    "    rows = Int[]\n",
    "    cols = Int[]\n",
    "    fracs = Float64[]\n",
    "    inGrid = false\n",
    "    coveredByGrid = false\n",
    "\n",
    "    if emissionType == \"point\"\n",
    "        if isnan(x) || isnan(y)\n",
    "            println(\"For point emissions, x and y coordinates must be provided\")\n",
    "            return nothing\n",
    "        end\n",
    "\n",
    "        point_wkt = \"POINT($x $y)\"\n",
    "        point = LibGEOS.readgeom(point_wkt)\n",
    "        pointInCells = Dict()\n",
    "\n",
    "        for (index, ext) in enumerate(grid.Extent)\n",
    "            coords_str = join([\"$(coord[1]) $(coord[2])\" for coord in ext], \", \")\n",
    "            cell_wkt = \"POLYGON(($coords_str))\"\n",
    "            cellPolygon = LibGEOS.readgeom(cell_wkt)\n",
    "\n",
    "            if LibGEOS.intersects(point, cellPolygon)\n",
    "                push!(pointInCells, index => cellPolygon)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        numIntersectingCells = length(pointInCells)\n",
    "        if numIntersectingCells > 0\n",
    "            inGrid = true\n",
    "            fraction = 1.0 / numIntersectingCells\n",
    "            for (idx, _) in pointInCells\n",
    "                push!(rows, idx)\n",
    "                push!(cols, 1)\n",
    "                push!(fracs, fraction)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        coveredByGrid = true\n",
    "    \n",
    "    elseif emissionType == \"area\"\n",
    "        countyPolygon, tgtCRS = findCountyPolygon(loc, countiesShapefile)\n",
    "        if countyPolygon === nothing || tgtCRS === nothing\n",
    "            println(\"Could not find Polygon or target CRS for location: $loc\")\n",
    "            return nothing\n",
    "        end\n",
    "        transformer = Proj.Transformation(gridCRS, tgtCRS)\n",
    "        totalArea = LibGEOS.area(countyPolygon)\n",
    "\n",
    "        for (index, ext) in enumerate(grid.Extent)\n",
    "            transformed_coords = [transformer(coord) for coord in ext]\n",
    "            transformed_coords_str = join([\"$(x) $(y)\" for (x, y) in transformed_coords], \", \")\n",
    "            cell_wkt = \"POLYGON(($transformed_coords_str))\"\n",
    "            cellPolygon = LibGEOS.readgeom(cell_wkt)\n",
    "\n",
    "            if LibGEOS.intersects(countyPolygon, cellPolygon)\n",
    "                intersection = LibGEOS.intersection(countyPolygon, cellPolygon)\n",
    "                cellArea = LibGEOS.area(intersection)\n",
    "                push!(fracs, cellArea / totalArea)\n",
    "                push!(rows, index)\n",
    "                push!(cols, 1)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if length(rows) > 0\n",
    "            inGrid = true\n",
    "        end\n",
    "        if sum(fracs) > 0.9999\n",
    "            coveredByGrid = true\n",
    "        end\n",
    "\n",
    "    else\n",
    "        println(\"Invalid emission type: $emissionType\")\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    return IndexInfo(rows, cols, fracs, inGrid, coveredByGrid)\n",
    "end\n",
    "\n",
    "\n",
    "# recordToGrid allocates the reciever to the specifed grid without any spatial surrogate.\n",
    "function recordToGrid(grid::GridDef, loc::String, countiesShapefile::String, srcCRS::String)\n",
    "    rows, cols, fracs, inGrid, coveredByGrid = GetIndex(grid, loc, countiesShapefile, srcCRS)\n",
    "    \n",
    "    gridSrg = spzeros(Float64, grid.Ny, grid.Nx)\n",
    "    \n",
    "    if inGrid\n",
    "        for i in eachindex(rows)\n",
    "            gridSrg[rows[i], cols[i]] += fracs[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return gridSrg, coveredByGrid, inGrid\n",
    "end\n",
    "\n",
    "# GridFactors returns the normalized fractions of emissions in each grid cell.\n",
    "function GridFactors(record::DataFrameRow, sp::SpatialProcessor, countiesShapefile::String, srcCRS::String)\n",
    "    loc = record.COUNTRY == \"USA\" ? \"US\" * record.FIPS : record.COUNTRY * record.FIPS\n",
    "    \n",
    "    return recordToGrid(sp.Grids, loc, countiesShapefile, srcCRS)\n",
    "end\n",
    "\n",
    "# uniqueCoordinates gets unique coordinates from records.\n",
    "function uniqueCoordinates(recs::Dict{String, Vector{DataFrameRow}})\n",
    "    uCoords = Set{Tuple{Float64, Float64}}()\n",
    "\n",
    "    for (scc, rows) in recs\n",
    "        for row in rows\n",
    "            if haskey(row, :LONGITUDE) && haskey(row, :LATITUDE)\n",
    "                lon_lat = (row.LONGITUDE, row.LATITUDE)\n",
    "                push!(uCoords, lon_lat)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return uCoords\n",
    "end\n",
    "\n",
    "# uniqueLoc gets unique locations (Country+FIPS) from records.\n",
    "function uniqueLoc(recs::Dict{String, Vector{DataFrameRow}})\n",
    "    uLocs = Set{String}()\n",
    "\n",
    "    for (scc, rows) in recs\n",
    "        for row in rows\n",
    "            country_fips = row.COUNTRY * row.FIPS  \n",
    "            push!(uLocs, country_fips)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return uLocs\n",
    "end\n",
    "\n",
    "# writeEmis writes emissions data to a shapefile.\n",
    "function writeEmis(filename::String, gd::GridDef, records::Dict{String, Vector{DataFrameRow}}, indexDict::Dict{Union{Tuple{Float64, Float64}, String}, Union{Nothing, IndexInfo}}, InMAP_crs::String, emissionType::String)    \n",
    "    df = DataFrame(\n",
    "        cellIndex = Int[],\n",
    "        VOC = Union{Float64, Missing}[],\n",
    "        NOX = Union{Float64, Missing}[],\n",
    "        NH3 = Union{Float64, Missing}[],\n",
    "        SO2 = Union{Float64, Missing}[],\n",
    "        PM25 = Union{Float64, Missing}[],\n",
    "        SCC = String[]\n",
    "    )\n",
    "\n",
    "    scc_index = Dict()\n",
    "\n",
    "    for (scc, scc_records) in records\n",
    "        for record in scc_records\n",
    "            pol = Symbol(record.POLID)\n",
    "            ann_value = record.ANN_VALUE\n",
    "            locIdx = nothing\n",
    "            \n",
    "            if emissionType == \"point\"\n",
    "                if haskey(record, :LONGITUDE) && haskey(record, :LATITUDE)\n",
    "                    longitude, latitude = record.LONGITUDE, record.LATITUDE\n",
    "                    locIdx = indexDict[(longitude, latitude)]\n",
    "                end\n",
    "            elseif emissionType == \"area\"\n",
    "                country = record.COUNTRY == \"USA\" ? \"US\" : record.COUNTRY\n",
    "                loc = country * record.FIPS\n",
    "                locIdx = indexDict[loc]\n",
    "            end\n",
    "\n",
    "            if locIdx !== nothing\n",
    "                for (idx, fraction) in zip(locIdx.rows, locIdx.fracs)\n",
    "                    emission_share = ustrip(ann_value * fraction)\n",
    "                    key = (idx, scc)\n",
    "\n",
    "                    if haskey(scc_index, key)\n",
    "                        row_index = scc_index[key]\n",
    "                        current_value = ismissing(df[row_index, pol]) ? 0.0 : df[row_index, pol]\n",
    "                        df[row_index, pol] = current_value + emission_share\n",
    "                    else\n",
    "                        new_row = DataFrame(\n",
    "                            cellIndex = [idx],\n",
    "                            VOC = missing, NOX = missing, NH3 = missing,\n",
    "                            SO2 = missing, PM25 = missing,\n",
    "                            SCC = [scc]\n",
    "                        )\n",
    "                        new_row[!, pol] = [emission_share]\n",
    "                        append!(df, new_row)\n",
    "                        scc_index[key] = size(df, 1)\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    function format_float(value)\n",
    "        if ismissing(value)\n",
    "            return \"0.0\"\n",
    "        else\n",
    "            return @sprintf(\"%.10e\", value)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    polygons = Vector{LibGEOS.Polygon}()\n",
    "    attributes = DataFrame(\n",
    "        cellIndex = Int[],\n",
    "        VOC = String[], NOX = String[], NH3 = String[],\n",
    "        SO2 = String[], PM25 = String[], SCC = String[]\n",
    "    )\n",
    "\n",
    "    for row in eachrow(df)\n",
    "        if row.cellIndex in keys(gd.Cells)\n",
    "            push!(polygons, gd.Cells[row.cellIndex])\n",
    "            push!(attributes, (\n",
    "                cellIndex = row.cellIndex,\n",
    "                VOC = format_float(row.VOC),\n",
    "                NOX = format_float(row.NOX),\n",
    "                NH3 = format_float(row.NH3),\n",
    "                SO2 = format_float(row.SO2),\n",
    "                PM25 = format_float(row.PM25),\n",
    "                SCC = row.SCC\n",
    "            ))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    writer = Shapefile.Writer(polygons, attributes)\n",
    "    Shapefile.write(filename, writer, force=true)\n",
    "\n",
    "    prj_filename = replace(filename, r\"\\.shp$\" => \"\") * \".prj\"\n",
    "    open(prj_filename, \"w\") do f\n",
    "        write(f, InMAP_crs)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# find_surrogate_by_code finds a surrogate by code and returns it or nothing if not found.\n",
    "function find_surrogate_by_code(srg_specs::Vector{SurrogateSpec}, code::Int)\n",
    "    for srg in srg_specs\n",
    "        if srg.Code == code\n",
    "            return srg\n",
    "        end\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\n",
    "# generate_data_sparse_matrices generates sparse matrices of counties within a specified extent from a county data shapefile.\n",
    "function generate_data_sparse_matrices(x_min, x_max, y_min, y_max, resolution, shapefile_path)\n",
    "    \n",
    "    ncols = round(Int, (x_max - x_min) / resolution)\n",
    "    nrows = round(Int, (y_max - y_min) / resolution)\n",
    "\n",
    "    domain_extent = Extent(X=(x_min, x_max), Y=(y_min, y_max))\n",
    "\n",
    "    shp = Shapefile.Table(shapefile_path)\n",
    "    shp_df = DataFrame(shp)\n",
    "\n",
    "    prj_path = replace(shapefile_path, \".shp\" => \".prj\")\n",
    "    source_crs_wkt = open(prj_path, \"r\") do file\n",
    "        read(file, String)\n",
    "    end\n",
    "\n",
    "    crs_obj = pyproj.CRS.from_wkt(source_crs_wkt)\n",
    "    epsg_code = crs_obj.to_epsg()\n",
    "\n",
    "    sparse_data_matrices = Dict{String, SparseMatrixCSC{Int, Int}}()\n",
    "\n",
    "    for row in eachrow(shp_df)\n",
    "        county_geometry = row.geometry\n",
    "\n",
    "        coords = GeoInterface.coordinates(county_geometry)\n",
    "        xs, ys = Float64[], Float64[]\n",
    "\n",
    "        for poly in coords\n",
    "            for ring in poly\n",
    "                append!(xs, [p[1] for p in ring])\n",
    "                append!(ys, [p[2] for p in ring])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if isempty(xs) || isempty(ys)\n",
    "            println(\"Skipping geometry with insufficient coordinates.\")\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        min_x, max_x = minimum(xs), maximum(xs)\n",
    "        min_y, max_y = minimum(ys), maximum(ys)\n",
    "\n",
    "        if max_x < x_min || min_x > x_max || max_y < y_min || min_y > y_max\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        x_start = max(round(Int, floor((min_x - x_min) / resolution)), 1)\n",
    "        x_end = min(round(Int, ceil((max_x - x_min) / resolution)), ncols)\n",
    "        y_start = max(round(Int, floor((min_y - y_min) / resolution)), 1)\n",
    "        y_end = min(round(Int, ceil((max_y - y_min) / resolution)), nrows)\n",
    "\n",
    "        county_extent = Extent(X=(min_x, max_x), Y=(min_y, max_y))\n",
    "\n",
    "        county_raster = rasterize(last, [county_geometry];\n",
    "            fill=1,\n",
    "            crs=EPSG(epsg_code),\n",
    "            extent=county_extent,\n",
    "            size=(y_end - y_start + 1, x_end - x_start + 1),\n",
    "            progress=false\n",
    "        )\n",
    "\n",
    "        raster_array = Matrix(county_raster)\n",
    "        replace!(raster_array, missing => 0)\n",
    "        raster_array .= map(x -> x > 0 ? 1 : 0, raster_array)\n",
    "\n",
    "        non_zero_indices = findall(x -> x == 1, raster_array)\n",
    "        adjusted_indices = [(y_start + i[1] - 1, x_start + i[2] - 1) for i in non_zero_indices]\n",
    "\n",
    "        sparse_matrix = spzeros(Int, nrows, ncols)\n",
    "        for (r, c) in adjusted_indices\n",
    "            sparse_matrix[r, c] = 1\n",
    "        end\n",
    "        sparse_data_matrices[row.GEOID] = sparse_matrix\n",
    "    end\n",
    "\n",
    "    return sparse_data_matrices\n",
    "end\n",
    "\n",
    "\n",
    "# generate_weight_sparse_matrices generates sparse weight matrices from a surrogate shapefile within a specified extent, with optional filtering.\n",
    "function generate_weight_sparse_matrices(\n",
    "    shapefile_path::String,\n",
    "    srg::SurrogateSpec,\n",
    "    ID2::Vector{String},\n",
    "    resolution::Float64,\n",
    "    x_min::Float64,\n",
    "    x_max::Float64,\n",
    "    y_min::Float64,\n",
    "    y_max::Float64;\n",
    "    Filter::Union{String, Nothing}=nothing,\n",
    ")\n",
    "\n",
    "    ncols = round(Int, (x_max - x_min) / resolution)\n",
    "    nrows = round(Int, (y_max - y_min) / resolution)\n",
    "\n",
    "    shp = Shapefile.Table(shapefile_path)\n",
    "    shp_df = DataFrame(shp)\n",
    "\n",
    "    prj_path = replace(shapefile_path, \".shp\" => \".prj\")\n",
    "    source_crs_wkt = open(prj_path, \"r\") do file\n",
    "        read(file, String)\n",
    "    end\n",
    "\n",
    "    pyproj = pyimport(\"pyproj\")\n",
    "    crs_obj = pyproj.CRS.from_wkt(source_crs_wkt)\n",
    "    epsg_code = crs_obj.to_epsg()\n",
    "\n",
    "    sparse_weight_matrices = Dict{Tuple{String, String}, SparseMatrixCSC{Int, Int}}()\n",
    "\n",
    "    if Filter !== nothing\n",
    "        filter_parts = split(Filter, \"=\")\n",
    "        if length(filter_parts) == 2\n",
    "            column, value = filter_parts[1], filter_parts[2]\n",
    "            if column == \"GRIDCODE\"\n",
    "                shp_df = filter(row -> row[column] == parse(Int, value), shp_df)\n",
    "            else\n",
    "                shp_df = filter(row -> row[column] == value, shp_df)\n",
    "            end\n",
    "        else\n",
    "            println(\"Invalid filter format. Skipping filter application.\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for row in eachrow(shp_df)\n",
    "        surrogate_geometry = row.geometry\n",
    "        coords = GeoInterface.coordinates(surrogate_geometry)\n",
    "        xs, ys = Float64[], Float64[]\n",
    "\n",
    "        for poly in coords\n",
    "            for ring in poly\n",
    "                append!(xs, [p[1] for p in ring])\n",
    "                append!(ys, [p[2] for p in ring])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if isempty(xs) || isempty(ys)\n",
    "            println(\"Skipping geometry with insufficient coordinates.\")\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        min_x, max_x = minimum(xs), maximum(xs)\n",
    "        min_y, max_y = minimum(ys), maximum(ys)\n",
    "\n",
    "        if max_x < x_min || min_x > x_max || max_y < y_min || min_y > y_max\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        x_start = max(round(Int, floor((min_x - x_min) / resolution)), 1)\n",
    "        x_end = min(round(Int, ceil((max_x - x_min) / resolution)), ncols)\n",
    "        y_start = max(round(Int, floor((min_y - y_min) / resolution)), 1)\n",
    "        y_end = min(round(Int, ceil((max_y - y_min) / resolution)), nrows)\n",
    "\n",
    "        surrogate_extent = Extent(X=(min_x, max_x), Y=(min_y, max_y))\n",
    "\n",
    "        surrogate_raster = rasterize(last, [surrogate_geometry];\n",
    "            fill=1,\n",
    "            crs=EPSG(epsg_code),\n",
    "            extent=surrogate_extent,\n",
    "            size=(y_end - y_start + 1, x_end - x_start + 1),\n",
    "            progress=false\n",
    "        )\n",
    "\n",
    "        raster_array = Matrix(surrogate_raster)\n",
    "        replace!(raster_array, missing => 0)\n",
    "        raster_array .= map(x -> x > 0 ? 1 : 0, raster_array)\n",
    "\n",
    "        sparse_matrix = spzeros(Int, nrows, ncols)\n",
    "        non_zero_indices = findall(x -> x == 1, raster_array)\n",
    "        adjusted_indices = [(y_start + i[1] - 1, x_start + i[2] - 1) for i in non_zero_indices]\n",
    "\n",
    "        for (r, c) in adjusted_indices\n",
    "            sparse_matrix[r, c] = 1\n",
    "        end\n",
    "\n",
    "        surrogate_id = string(row[ID2[1]])\n",
    "        grid_code = string(row[ID2[2]])\n",
    "\n",
    "        sparse_weight_matrices[(surrogate_id, grid_code)] = sparse_matrix\n",
    "    end\n",
    "\n",
    "    return sparse_weight_matrices\n",
    "end\n",
    "\n",
    "# generate_grid_sparse_matrices generates sparse matrices of grid cells within a specified extent from a grid definition.\n",
    "function generate_grid_sparse_matrices(\n",
    "    x_min::Float64,\n",
    "    x_max::Float64,\n",
    "    y_min::Float64,\n",
    "    y_max::Float64,\n",
    "    resolution::Float64,\n",
    "    gd::GridDef\n",
    ")\n",
    "    pyproj = pyimport(\"pyproj\")\n",
    "    wgs84 = \"+proj=longlat +datum=WGS84 +no_defs\"\n",
    "    source_crs = pyproj.CRS(gd.SR)\n",
    "    target_crs = pyproj.CRS(wgs84)\n",
    "    transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=true)\n",
    "    ncols = round(Int, (x_max - x_min) / resolution)\n",
    "    nrows = round(Int, (y_max - y_min) / resolution)\n",
    "    domain_extent = Extent(X=(x_min, x_max), Y=(y_min, y_max))\n",
    "    sparse_grid_cell_matrices = Dict{String, SparseMatrixCSC{Int, Int}}()\n",
    "    for (i, extent) in enumerate(gd.Extent)\n",
    "        transformed_coords = [transformer.transform(x, y) for (x, y) in extent]\n",
    "        xs = [p[1] for p in transformed_coords]\n",
    "        ys = [p[2] for p in transformed_coords]\n",
    "        if isempty(xs) || isempty(ys)\n",
    "            println(\"Skipping grid cell with insufficient coordinates.\")\n",
    "            continue\n",
    "        end\n",
    "        min_x, max_x = minimum(xs), maximum(xs)\n",
    "        min_y, max_y = minimum(ys), maximum(ys)\n",
    "        if max_x < x_min || min_x > x_max || max_y < y_min || min_y > y_max\n",
    "            continue\n",
    "        end\n",
    "        x_start = max(round(Int, floor((min_x - x_min) / resolution)), 1)\n",
    "        x_end = min(round(Int, ceil((max_x - x_min) / resolution)), ncols)\n",
    "        y_start = max(round(Int, floor((min_y - y_min) / resolution)), 1)\n",
    "        y_end = min(round(Int, ceil((max_y - y_min) / resolution)), nrows)\n",
    "        size_tuple = (y_end - y_start + 1, x_end - x_start + 1)\n",
    "        grid_cell_extent = Extent(X=(min_x, max_x), Y=(min_y, max_y))\n",
    "        grid_cell_geometry = gd.Cells[i]\n",
    "        grid_cell_raster = rasterize(last, [grid_cell_geometry];\n",
    "            fill=1,\n",
    "            crs=EPSG(4326),\n",
    "            extent=grid_cell_extent,\n",
    "            size=size_tuple,\n",
    "            progress=false\n",
    "        )\n",
    "        raster_array = Matrix(grid_cell_raster)\n",
    "        replace!(raster_array, missing => 0)\n",
    "        raster_array .= map(x -> x > 0 ? 1 : 0, raster_array)\n",
    "        non_zero_indices = findall(x -> x == 1, raster_array)\n",
    "        adjusted_indices = [(y_start + idx[1] - 1, x_start + idx[2] - 1) for idx in non_zero_indices]\n",
    "        sparse_matrix = spzeros(Int, nrows, ncols)\n",
    "        for (r, c) in adjusted_indices\n",
    "            sparse_matrix[r, c] = 1\n",
    "        end\n",
    "        sparse_grid_cell_matrices[string(i)] = sparse_matrix\n",
    "    end\n",
    "    return sparse_grid_cell_matrices\n",
    "end\n",
    "\n",
    "# generate_countySurrogate generates county surrogates by combining sparse data and weight matrices.\n",
    "function generate_countySurrogate(sparse_data_matrices, sparse_weight_matrices)\n",
    "    index_to_weight_keys = Dict{Tuple{Int, Int}, Vector{Tuple{Tuple{String, String}, Int64}}}()\n",
    "    for wkey in keys(sparse_weight_matrices)\n",
    "        wmat = sparse_weight_matrices[wkey]\n",
    "        I_w, J_w, V_w = findnz(wmat)\n",
    "        for idx in 1:length(V_w)\n",
    "            i = I_w[idx]\n",
    "            j = J_w[idx]\n",
    "            index = (i, j)\n",
    "            value = V_w[idx]\n",
    "            if haskey(index_to_weight_keys, index)\n",
    "                push!(index_to_weight_keys[index], (wkey, value))\n",
    "            else\n",
    "                index_to_weight_keys[index] = [(wkey, value)]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    countySurrogate = Dict{String, Tuple{Float64, Vector{Vector{String}}}}()\n",
    "    for dkey in keys(sparse_data_matrices)\n",
    "        dmat = sparse_data_matrices[dkey]\n",
    "        I_d, J_d, V_d = findnz(dmat)\n",
    "        total_sum = 0.0\n",
    "        nonzero_weight_keys = Set{Tuple{String, String}}()\n",
    "        for idx in 1:length(V_d)\n",
    "            i = I_d[idx]\n",
    "            j = J_d[idx]\n",
    "            data_value = V_d[idx]\n",
    "            index = (i, j)\n",
    "            if haskey(index_to_weight_keys, index)\n",
    "                for (wkey, weight_value) in index_to_weight_keys[index]\n",
    "                    product = data_value * weight_value\n",
    "                    total_sum += product\n",
    "                    push!(nonzero_weight_keys, wkey)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        weight_keys_list = [ [wkey[1], wkey[2]] for wkey in nonzero_weight_keys ]\n",
    "        countySurrogate[dkey] = (total_sum, weight_keys_list)\n",
    "    end\n",
    "    return countySurrogate\n",
    "end\n",
    "\n",
    "\n",
    "# update_locIndex generates updated locIndex by recalculating values using county surrogates.\n",
    "function update_locIndex(\n",
    "    locIndex,\n",
    "    sparse_data_matrices,\n",
    "    sparse_weight_matrices,\n",
    "    sparse_grid_matrices,\n",
    "    countySurrogate\n",
    ")    \n",
    "    updated_locIndex = Dict{Union{Tuple{Float64, Float64}, String}, Union{IndexInfo, Nothing}}()\n",
    "\n",
    "    for (fips, indexInfo) in locIndex\n",
    "        if isnothing(indexInfo)\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        data_fips = fips[3:end]\n",
    "        cell_indices = indexInfo.rows\n",
    "        num_cells = length(cell_indices)\n",
    "        new_values = Vector{Float64}(undef, num_cells)\n",
    "        \n",
    "        county_matrix = get(sparse_data_matrices, data_fips, spzeros(0, 0))\n",
    "        total_sum, surrogates = get(countySurrogate, data_fips, (0.0, []))\n",
    "        \n",
    "        if nnz(county_matrix) == 0 || total_sum == 0.0\n",
    "            new_values .= 0.0\n",
    "            updated_locIndex[fips] = IndexInfo(\n",
    "                cell_indices,\n",
    "                fill(1, num_cells),\n",
    "                new_values,\n",
    "                indexInfo.inGrid,\n",
    "                indexInfo.coveredByGrid\n",
    "            )\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        cached_weight_matrices = Dict{Tuple{String, String}, SparseMatrixCSC{Float64, Int}}()\n",
    "        for surrogate in surrogates\n",
    "            surrogate_id, gridcode = surrogate\n",
    "            surrogate_id_int = isa(surrogate_id, Int) ? surrogate_id : parse(Int, surrogate_id)\n",
    "            gridcode_int = isa(gridcode, Int) ? gridcode : parse(Int, gridcode)\n",
    "            surrogate_key = (string(surrogate_id_int), string(gridcode_int))\n",
    "            if haskey(sparse_weight_matrices, surrogate_key)\n",
    "                cached_weight_matrices[surrogate_key] = sparse_weight_matrices[surrogate_key]\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if isempty(cached_weight_matrices)\n",
    "            new_values .= 0.0\n",
    "            updated_locIndex[fips] = IndexInfo(\n",
    "                cell_indices,\n",
    "                fill(1, num_cells),\n",
    "                new_values,\n",
    "                indexInfo.inGrid,\n",
    "                indexInfo.coveredByGrid\n",
    "            )\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        Threads.@threads for idx in 1:num_cells\n",
    "            cell_index = string(cell_indices[idx])\n",
    "            cell_value = 0.0\n",
    "\n",
    "            if haskey(sparse_grid_matrices, cell_index)\n",
    "                cell_matrix = sparse_grid_matrices[cell_index]\n",
    "                if nnz(cell_matrix) > 0\n",
    "                    combined_matrix = cell_matrix .* county_matrix\n",
    "                    if nnz(combined_matrix) > 0\n",
    "                        for weight_matrix in values(cached_weight_matrices)\n",
    "                            if nnz(weight_matrix) > 0\n",
    "                                product_sum = dot(combined_matrix, weight_matrix)\n",
    "                                cell_value += product_sum\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            normalized_value = cell_value / total_sum\n",
    "            new_values[idx] = normalized_value\n",
    "        end\n",
    "\n",
    "        updated_locIndex[fips] = IndexInfo(\n",
    "            cell_indices,\n",
    "            fill(1, num_cells),\n",
    "            new_values,\n",
    "            indexInfo.inGrid,\n",
    "            indexInfo.coveredByGrid\n",
    "        )\n",
    "    end\n",
    "\n",
    "    return updated_locIndex\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Next, we need to specify which files we are going to load and how we are going to load them.\n",
    "There are several different file formats, and we need to specify what format each of the files is in.\n",
    "For now, we are focusing on the files that contain annual emissions (rather than daily or hourly emissions\n",
    "or activity data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "readpoint(f) = FF10PointDataFrame(CSV.read(joinpath(f), DataFrame, comment=\"#\"))\n",
    "readnonpoint(f) = FF10NonPointDataFrame(CSV.read(joinpath(f), DataFrame, comment=\"#\"))\n",
    "readnonroad(f) = FF10NonRoadDataFrame(CSV.read(joinpath(f), DataFrame, comment=\"#\"))\n",
    "readonroad(f) = FF10OnRoadDataFrame(CSV.read(joinpath(f), DataFrame, comment=\"#\"))\n",
    "\n",
    "emisdirs = (\n",
    "     \"2019ge_cb6_19k/inputs/afdust\" => (\n",
    "        \"2019ge_from_afdust_2017NEIpost_NONPOINT_20210129_04nov2021_v0.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/airports\" => (\n",
    "        \"2019ge_from_airports_SmokeFlatFile_2019NEI_POINT_20210914_runway_apportion_25jan2022_nf_v1.csv\" => readpoint,\n",
    "    ),\n",
    "    # Format is FF10_POINT although first comment is: #FORMAT=FF10_NONPOINT\n",
    "    \"2019ge_cb6_19k/inputs/canada_ag\" => (\n",
    "        \"canada_MYR_2016_ag_animal_NH3_VOC_monthly_12km_aggregated_09jun2021_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_ag_fertilizer_NH3_monthly_12km_16apr2021_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/canada_og2D\" => (\n",
    "        \"canada_MYR_2016_point_UOG_15jun2021_nf_v1.csv\" => readpoint,\n",
    "    ),\n",
    "    #\"2019ge_cb6_19k/inputs/cem\" => (    \n",
    "    #),\n",
    "    \"2019ge_cb6_19k/inputs/cmv_c1c2_12\" => (\n",
    "        \"cmv_c1c2_2019_12US1_2019_CA_annual_11feb2022_v0.csv\" => readpoint,\n",
    "        \"cmv_c1c2_2019_12US1_2019_MX_annual_11feb2022_v0.csv\" => readpoint,\n",
    "        \"cmv_c1c2_2019_12US1_2019_US_annual_11feb2022_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/cmv_c3_12\" => (\n",
    "        \"cmv_c3_2019_masked_12US1_2019_CA_annual_10feb2022_v0.csv\" => readpoint,\n",
    "        \"cmv_c3_2019_masked_12US1_2019_MX_annual_10feb2022_v0.csv\" => readpoint,\n",
    "        \"cmv_c3_2019_masked_12US1_2019_US_annual_10feb2022_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/fertilizer\" => (\n",
    "        \"2019ge_fertilizer_NH3_monthly_23jun2022_v0.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/livestock\" => (\n",
    "        \"2019ge_from_livestock_2017NEIpost_NONPOINT_20210129_29nov2021_nf_v1.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/nonpt\" => (\n",
    "        \"2017NEIpost_NONPOINT_20210129_08oct2021_v2.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/nonroad\" => (\n",
    "        \"2019_interpolation_texas_nonroad_29oct2021_v1.csv\" => readnonroad,\n",
    "        \"2019ge_from_2017nei_nonroad_california_29oct2021_v0.csv\" => readnonroad,\n",
    "        \"2019ge_nonroad_from_MOVES_aggSCC_29oct2021_v1.csv\" => readnonroad, \n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/np_oilgas\" => (\n",
    "        \"np_oilgas_2019ge_02mar2022_nf_v3.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/np_solvents\" => (\n",
    "        \"nonVCPy_solvents_2017NEIpost_NONPOINT_20210129_15feb2022_nf_v1.csv\" => readnonpoint,\n",
    "        \"solvents_2017NEIpost_NONPOINT_20210129_nonVCPy_polls_for2016v3_2019ge_15feb2022_v0.csv\" => readnonpoint,\n",
    "        \"VCPy_solvents_2019ge_minuspoint_21feb2022_nf_v2.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/onroad\" => (\n",
    "        \"2019ge_SMOKE_MOVES_MOVES3_AQ_fullHAP_30nov2021_v0.csv\" => readonroad,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/onroad_can\" => (\n",
    "        \"canada_2019ge_projection_T3_onroad_monthly_14mar2022_v0.csv\" => readonroad,\n",
    "        \"canada_2019ge_projection_T3_onroad_refueling_14mar2022_v0.csv\" => readonroad,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/onroad_mex\" => (\n",
    "        \"Mexico_2019_onroad_MOVES_interpolated_22oct2021_v0.csv\" => readonroad,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/othafdust\" => (\n",
    "        \"canada_MYR_2016_area_dust_27dec2020_v0.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/othar\" => (\n",
    "        \"2019ge_proj_CEDS_from_Mexico_2016INEM_nonpoint_09nov2021_v0.csv\" => readnonpoint,\n",
    "         \"2019ge_proj_CEDS_from_Mexico_2016INEM_nonroad_09nov2021_v0.csv\" => readnonroad,\n",
    "        \"canada_2019ge_proj_from2016_T4_nonroad_monthly_15mar2022_v0.csv\" => readnonroad,\n",
    "        \"canada_MYR_2016_area_other_27dec2020_v0.csv\" => readnonpoint,\n",
    "        \"canada_MYR_2016_T5_rail_27dec2020_v0.csv\" => readnonpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/othpt\" => (\n",
    "        \"2019ge_proj_CEDS_from_Mexico_2016_point_20191209_06mar2023_nf_v3.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_point_CB6VOC_monthly_27dec2020_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_point_nodust_noVOC_monthly_27dec2020_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_point_UOG_15jun2021_nf_v2.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_point_VOC_INV_monthly_27dec2020_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_T1_airports_monthly_27dec2020_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/othptdust\" => (\n",
    "        \"canada_MYR_2016_ag_animal_PM10_monthly_12km_27dec2020_v0.csv\" => readpoint,\n",
    "         \"canada_MYR_2016_ag_animal_PM25_monthly_12km_27dec2020_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_ag_harvest_monthly_12km_27dec2020_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_ag_tillage_monthly_12km_27dec2020_v0.csv\" => readpoint,\n",
    "        \"canada_MYR_2016_point_source_dust_monthly_27dec2020_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/pt_oilgas\" => (\n",
    "        \"2019ge_from_oilgas_2019NEI_SmokeFlatFile_POINT_20220325_calcyear2017_07apr2022_v0.csv\" => readpoint,\n",
    "        \"oilgas_2019NEI_SmokeFlatFile_POINT_20220325_calcyear2019_06apr2022_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/ptagfire\" => (\n",
    "        \"ptinv_agfire_CONUS_2019ge_23dec2021_v0.csv\" => readpoint,\n",
    "        \"ptinv_agfire_FL_2019ge_23dec2021_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/ptegu\" => (\n",
    "        \"egucems_2019NEI_SmokeFlatFile_POINT_20220325_07apr2022_v0.csv\" => readpoint,\n",
    "        \"egunoncems_2019NEI_SmokeFlatFile_POINT_20220325_08apr2022_nf_v1.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/ptfire_othna\" => (\n",
    "        \"ptinv_finn_CA_finn_2019ge_ff10_26oct2021_v0.csv\" => readpoint,\n",
    "        \"ptinv_finn_MX_finn_2019ge_ff10_26oct2021_v0.csv\" => readpoint,\n",
    "        \"ptinv_finn_ONA_finn_2019ge_ff10_26oct2021_v0.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/ptfire-rx\" => (\n",
    "        \"ptinv_ptfire_FH_grassland_2019ge_23dec2021_v0.csv\" => readpoint,\n",
    "        \"ptinv_ptfire_sf2_2019ge_bsp_16dec2021_nf_v1.csv\" => readpoint,\n",
    "        \"ptinv_ptfire_sf2_2019ge_bsp_haps_16dec2021_nf_v1.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/ptfire-wild\" => (\n",
    "        \"ptinv_ptfire_sf2_2019ge_bsp_16dec2021_nf_v2.csv\" => readpoint,\n",
    "        \"ptinv_ptfire_sf2_2019ge_bsp_haps_16dec2021_nf_v2.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/ptnonipm\" => (\n",
    "        \"2019_eto_ptnonipm_supplemental_07apr2022_nf_v1.csv\" => readpoint,\n",
    "        \"nonegu_2019NEI_SmokeFlatFile_POINT_20220325_07apr2022_nf_v1.csv\" => readpoint,\n",
    "        \"railyard_postprojection_2019NEI_SmokeFlatFile_POINT_20211221_05apr2022_nf_v1.csv\" => readpoint,\n",
    "    ),\n",
    "    \"2019ge_cb6_19k/inputs/rail\" => (\n",
    "        \"2019ge_from_rail_2017NEIpost_NONPOINT_20210129_04jan2022_v0.csv\" => readnonpoint,\n",
    "    ),\n",
    "    # CONUS Only\n",
    "    \"2019ge_cb6_19k/inputs/rwc\" => (\n",
    "        \"rwc_2017NEIpost_NONPOINT_20210129_06apr2022_nf_v6.csv\" => readnonpoint,\n",
    "    ),    \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load emissions and generate shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example for rwc. We will update it to loop over all sectors.\n",
    "\n",
    "function main()\n",
    "    x_min = -138.0\n",
    "    x_max = -58.0\n",
    "    y_min = 17.0\n",
    "    y_max = 57.0\n",
    "    resolution = 0.005\n",
    "\n",
    "\n",
    "    base_dir = \"/path/to/your/project\"\n",
    "    \n",
    "    c = Config([\n",
    "        joinpath(base_dir, \"gridding\", \"agref_us_2017platform_13jan2022_nf_v12.txt\"),\n",
    "        joinpath(base_dir, \"gridding\", \"amgref_can2015_mex2010_for2016beta_09mar2021_nf_v5.txt\"),\n",
    "        joinpath(base_dir, \"gridding\", \"mgref_onroad_MOVES3_23sep2021_v1.txt\")\n",
    "    ],\n",
    "        joinpath(base_dir, \"Scripts\", \"surrogate_specification_2019.csv\"),\n",
    "        joinpath(base_dir, \"Surrogates\"),\n",
    "        \"+proj=longlat\",\n",
    "        \"+proj=lcc +lat_1=33 +lat_2=45 +lat_0=40 +lon_0=-97 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +to_meter=1\",\n",
    "        joinpath(base_dir, \"gridCells.txt\"),\n",
    "        \"InMAP\",\n",
    "        joinpath(base_dir, \"data\", \"cb_2019_us_county_500k_WGS84.shp\"),\n",
    "        joinpath(base_dir, \"Emission Shapefiles\", \"2019\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    InMAP_crs = \"\"\"\n",
    "    PROJCS[\\\"Lambert_Conformal_Conic\\\",GEOGCS[\\\"GCS_unnamed ellipse\\\",DATUM[\\\"D_unknown\\\",SPHEROID[\\\"locIndexUnknown\\\",6370997.000000,0]],PRIMEM[\\\"Greenwich\\\",0],UNIT[\\\"Degree\\\",0.017453292519943295]],PROJECTION[\\\"Lambert_Conformal_Conic\\\"],PARAMETER[\\\"standard_parallel_1\\\",33],PARAMETER[\\\"standard_parallel_2\\\",45],PARAMETER[\\\"latitude_of_origin\\\",40],PARAMETER[\\\"central_meridian\\\",-97],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"Meter\\\",1]]\n",
    "    \"\"\"\n",
    "\n",
    "    sp, gd = setupSpatialProcessor(c)\n",
    "\n",
    "    gdCRS = \"+proj=lcc +lat_1=33 +lat_2=45 +lat_0=40 +lon_0=-97 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +to_meter=1\"\n",
    "    emisCRS = \"EPSG:4326\" \n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for (emisdir, data) in emisdirs\n",
    "        for (file, readfunc) in data\n",
    "            path = joinpath(dir, emisdir, file)\n",
    "            df = readfunc(path)\n",
    "            \n",
    "            if readfunc === readpoint\n",
    "                push!(dfs, df.df[!, [:POLID, :COUNTRY, :FIPS, :SCC, :ANN_VALUE, :LONGITUDE, :LATITUDE]])\n",
    "            elseif readfunc === readnonpoint || readfunc === readnonroad || readfunc === readonroad\n",
    "                area_df = df.df[!, [:POLID, :COUNTRY, :FIPS, :SCC, :ANN_VALUE]]\n",
    "                area_df[!, :LONGITUDE] = fill(missing, nrow(area_df))\n",
    "                area_df[!, :LATITUDE] = fill(missing, nrow(area_df))\n",
    "                push!(dfs, area_df)\n",
    "            else\n",
    "                println(\"Unknown emission type\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    emis = vcat(dfs...)\n",
    "    emis_grouped = sort(DataFrames.combine(DataFrames.groupby(emis, [:POLID, :COUNTRY, :FIPS, :SCC, :LONGITUDE, :LATITUDE]), :ANN_VALUE => sum => :ANN_VALUE), :ANN_VALUE, rev=true)\n",
    "\n",
    "    filtered_emis_grouped = filter(row -> row.POLID in keys(Pollutants), emis_grouped)\n",
    "\n",
    "    joined_data = leftjoin(filtered_emis_grouped, sp.GridRef, on=[:COUNTRY, :FIPS, :SCC], makeunique=true)\n",
    "\n",
    "    joined_data_1 = filter(row -> ismissing(row.Surrogate), eachrow(joined_data))\n",
    "    joined_data_1 = DataFrame(joined_data_1)\n",
    "    select!(joined_data_1, Not(:Surrogate))\n",
    "    original_FIPS = joined_data_1[!, :FIPS]\n",
    "    joined_data_1[!, :FIPS] .= \"00000\"\n",
    "    joined_data_1 = leftjoin(joined_data_1, select(sp.GridRef, [:FIPS, :SCC, :Surrogate]), on=[:FIPS, :SCC], makeunique=true)\n",
    "    joined_data_1[!, :FIPS] .= original_FIPS\n",
    "\n",
    "    joined_data_2 = filter(row -> !ismissing(row.Surrogate), eachrow(joined_data))\n",
    "    joined_data_2 = DataFrame(joined_data_2)\n",
    "\n",
    "    filtered_emis_grouped = vcat(joined_data_1, joined_data_2)\n",
    "\n",
    "    for row in eachrow(filtered_emis_grouped)\n",
    "        row.POLID = Pollutants[row.POLID]\n",
    "    end\n",
    "\n",
    "    records = Dict{String, Vector{DataFrameRow}}()\n",
    "    for row in eachrow(filtered_emis_grouped)\n",
    "        scc = row.SCC\n",
    "        if haskey(records, scc)\n",
    "            push!(records[scc], row)\n",
    "        else\n",
    "            records[scc] = [row]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    uLocs = uniqueLoc(records)\n",
    "\n",
    "    locIndex = Dict{Union{Tuple{Float64, Float64}, String}, Union{IndexInfo, Nothing}}()\n",
    "    for loc in uLocs\n",
    "        idx = GetIndex(gd, gdCRS, \"area\"; loc=loc, countiesShapefile=c.Counties)\n",
    "        locIndex[loc] = idx\n",
    "    end\n",
    "\n",
    "    sparse_data_matrices = generate_data_sparse_matrices(x_min, x_max, y_min, y_max, resolution, c.Counties)\n",
    "\n",
    "    weight_shapefile = joinpath(base_dir, \"Surrogates\", \"CONUS_AK_NLCD_2011_500m_WGS84_2.shp\")  # For testing\n",
    "    srg = find_surrogate_by_code(sp.SrgSpecs, 300)\n",
    "    ID2 = [\"ID\", \"GRIDCODE\"]\n",
    "\n",
    "    sparse_weight_matrices = generate_weight_sparse_matrices(\n",
    "        weight_shapefile,\n",
    "        srg,\n",
    "        ID2,\n",
    "        resolution,\n",
    "        x_min,\n",
    "        x_max,\n",
    "        y_min,\n",
    "        y_max;\n",
    "        Filter=\"GRIDCODE=22\",\n",
    "    )\n",
    "\n",
    "    sparse_grid_matrices = generate_grid_sparse_matrices(\n",
    "        x_min,\n",
    "        x_max,\n",
    "        y_min,\n",
    "        y_max,\n",
    "        resolution,\n",
    "        gd\n",
    "    )\n",
    "\n",
    "    countySurrogate = generate_countySurrogate(sparse_data_matrices, sparse_weight_matrices)\n",
    "\n",
    "    locIndex_s = update_locIndex(locIndex, sparse_data_matrices, sparse_weight_matrices, sparse_grid_matrices, countySurrogate)\n",
    "\n",
    "    writeEmis(c.EmisShp * \"/rwc.shp\", gd, records, locIndex_s, InMAP_crs, \"area\")\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
